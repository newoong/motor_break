{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWnqdB3Y7HuL"
      },
      "source": [
        "# TimeGAN - Hands On Lab\n",
        "\n",
        "### Introduction\n",
        "A good generative model for time-series data should preserve temporal dynamics,\n",
        "in the sense that new sequences respect the original relationships between variables across time. Existing methods that bring generative adversarial networks into the sequential setting do not adequately attend to the temporal correlations unique to time-series data. At the same time, supervised models for sequence prediction which allow finer control over network dynamics are inherently deterministic. \n",
        "\n",
        "TimeGAN has combined the flexibility of the unsupervised paradigm with the control afforded by supervised training to generate realistic time-series data. For this purpose, a new modules was added which is the autoencoder. This will part in accomplishing the mission of learning an embedding space jointly optimized with both supervised and adversarial objectives.\n",
        "\n",
        "Released paper in December 2019 presented at NeurIPS by Yoon, Jarrett, and van der Schaar. Paper link https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks.pdf\n",
        "\n",
        "### Learning Objectives\n",
        "* Learn the timeGAN network design\n",
        "* Training TimeGAN\n",
        "* Explore Evaluation methods for timeGAN \n",
        "\n",
        "### Project Workflow\n",
        "\n",
        "1. Selecting and preparing real and random time series inputs\n",
        "2. Creating the key TimeGAN model components\n",
        "3. Defining the various loss functions and training steps used during the three training\n",
        "phases\n",
        "4. Running the training loops and logging the results\n",
        "5. Generating synthetic time series and evaluating the results\n",
        "\n",
        "### Steps to run this notebook from Colaboratory\n",
        "\n",
        "This colab will run much faster on GPU. To use a Google Cloud\n",
        "GPU:\n",
        "\n",
        "1. Go to `Runtime > Change runtime type`.\n",
        "1. Click `Hardware accelerator`.\n",
        "1. Select `GPU` and click `Save`.\n",
        "1. Click `Connect` in the upper right corner and select `Connect to hosted runtime`.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vh8mtY-T6na"
      },
      "source": [
        "### Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlsIYgEP7HuR",
        "outputId": "65e5c37c-739b-4a48-848f-ca9a3dbff179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting quandl\n",
            "  Downloading Quandl-3.7.0-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: pandas>=0.14 in /usr/local/lib/python3.7/dist-packages (from quandl) (1.3.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from quandl) (9.0.0)\n",
            "Collecting inflection>=0.3.1\n",
            "  Downloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from quandl) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from quandl) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from quandl) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from quandl) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.14->quandl) (2022.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (3.0.4)\n",
            "Installing collected packages: inflection, quandl\n",
            "Successfully installed inflection-0.5.1 quandl-3.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm\n",
        "#!pip install pandas_datareader==0.7.0\n",
        "#!pip install pandas-datareader --upgrade\n",
        "#!pip install tables\n",
        "!pip install quandl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jR6CRWpT7tj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c2f0de-eed7-46a6-e9d7-e3ca626e86e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZPU5sC_7HuM"
      },
      "source": [
        "### IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4FuV7DU7HuV"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import random\n",
        "from pandas_datareader import data\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import quandl\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import GRU, Dense, RNN, GRUCell, Input\n",
        "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('white')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrVpNVCJ7HuY"
      },
      "source": [
        "### CONFIGS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols=['Ia', 'Ib', 'Ic', 'Idc', 'RPM', 'Torque', 'Ia_RMS', 'Ib_RMS', 'Ic_RMS']\n",
        "portent_p=pd.read_excel('/content/drive/MyDrive/IPM 모터 고장 실험(상개방, interturn short)데이터_2000RPM_1ms.xlsx',header=1,sheet_name=1,usecols=cols)\n",
        "portent_i=pd.read_excel('/content/drive/MyDrive/IPM 모터 고장 실험(상개방, interturn short)데이터_2000RPM_1ms.xlsx',header=1,sheet_name=2,usecols=cols)\n",
        "portent_p=portent_p.iloc[:160,:]\n",
        "portent_i=portent_i.iloc[:160,:]\n",
        "print(portent_p.shape,portent_i.shape)"
      ],
      "metadata": {
        "id": "ohv8I7v6ylN7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9b93a14-30cb-434b-e19b-7783065526d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(160, 9) (160, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "portent_p = tf.data.Dataset.from_tensor_slices(portent_p)\n",
        "portent_p = portent_p.window(24,shift=1,drop_remainder=True)\n",
        "list_p = []\n",
        "for d in portent_p:\n",
        "    i2 = list(d.as_numpy_iterator())\n",
        "    list_p.append(i2)\n",
        "\n",
        "portent_p = np.array(list_p)\n",
        "print(portent_p.shape)\n",
        "\n",
        "portent_i = tf.data.Dataset.from_tensor_slices(portent_i)\n",
        "portent_i = portent_i.window(24,shift=1,drop_remainder=True)\n",
        "list_i = []\n",
        "for d in portent_i:\n",
        "    i2 = list(d.as_numpy_iterator())\n",
        "    list_i.append(i2)\n",
        "\n",
        "portent_i = np.array(list_i)\n",
        "print(portent_i.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg8gmzps3EgI",
        "outputId": "8926ded7-fea8-47de-87dd-0dc8bf1798e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(137, 24, 9)\n",
            "(137, 24, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Xp_train,Xp_test=train_test_split(portent_p,random_state=42,shuffle=True,test_size=0.2)\n",
        "Xi_train,Xi_test=train_test_split(portent_i,random_state=42,shuffle=True,test_size=0.2)"
      ],
      "metadata": {
        "id": "Te8LzqXT3KrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler=StandardScaler()\n",
        "Xp=scaler.fit_transform(Xp_train.reshape(-1,9)).reshape(-1,24,9).astype(np.float32)\n",
        "Xi=scaler.fit_transform(Xi_train.reshape(-1,9)).reshape(-1,24,9).astype(np.float32)"
      ],
      "metadata": {
        "id": "vqMUyuYjw2Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkOOCj8f7HuZ"
      },
      "outputs": [],
      "source": [
        "# Experiments path\n",
        "results_path = Path('time_gan')\n",
        "if not results_path.exists():\n",
        "    results_path.mkdir()\n",
        "\n",
        "experiment = 1\n",
        "log_dir = results_path / f'experiment_{experiment:02}'\n",
        "if not log_dir.exists():\n",
        "    log_dir.mkdir(parents=True)\n",
        "hdf_store = results_path / 'TimeSeriesGAN.h5'\n",
        "\n",
        "\n",
        "\n",
        "# Model Hyperparameters\n",
        "seq_len = 24 #window 크기\n",
        "n_seq = 9 #변수개수\n",
        "batch_size = 8\n",
        "hidden_dim = 24\n",
        "num_layers = 3\n",
        "train_steps = 10000\n",
        "gamma = 1\n",
        "\n",
        "# Data\n",
        "'''api_key = \"Your API KEY\"\n",
        "start_date = '1990-01-01'\n",
        "end_date = '2019-02-01'\n",
        "\n",
        "# A fixed random seed is a common \"trick\" used in ML that allows us to recreate\n",
        "# the same data when there is a random element involved.'''\n",
        "seed = random.seed(30)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9ptkgIQ-GBm",
        "outputId": "8e49b0a2-9369-499b-acdb-a77930daf437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4zV8L8M7Hu1"
      },
      "outputs": [],
      "source": [
        "n_windows=len(Xi)\n",
        "real_series = (tf.data.Dataset.from_tensor_slices(Xi).shuffle(buffer_size=n_windows).batch(batch_size))\n",
        "real_series_iter = iter(real_series.repeat())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRjT13bM7Hu4"
      },
      "outputs": [],
      "source": [
        "# Set up Random Series Generator\n",
        "def make_random_data():\n",
        "    while True:\n",
        "        yield np.random.uniform(low=0, high=1, size=(seq_len, n_seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g43AO4F17Hu7"
      },
      "outputs": [],
      "source": [
        "random_series = iter(tf.data.Dataset.from_generator(make_random_data, output_types=tf.float32).batch(batch_size).repeat())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxCjX_0o7HvB"
      },
      "outputs": [],
      "source": [
        "# Fix model inputs\n",
        "X = Input(shape=[seq_len, n_seq], name='RealData')\n",
        "Z = Input(shape=[seq_len, n_seq], name='RandomData')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypVLjPHc7Hu-"
      },
      "source": [
        "### Modeling\n",
        "![Discriminator.png](https://drive.google.com/uc?export=view&id=1NeG9CjB7hfAg7TlTlWY_kbGSLxAihHXd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwUfluKK7HvE"
      },
      "outputs": [],
      "source": [
        "### RNN block generator\n",
        "def make_rnn(n_layers, hidden_units, output_units, name):\n",
        "    return Sequential([GRU(units=hidden_units,\n",
        "                           return_sequences=True,\n",
        "                           name=f'GRU_{i + 1}') for i in range(n_layers)] +\n",
        "                      [Dense(units=output_units,\n",
        "                             activation='sigmoid',\n",
        "                             name='OUT')], name=name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqEwvD3H7HvH"
      },
      "source": [
        "#### AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1A6nf8Jt7HvI"
      },
      "outputs": [],
      "source": [
        "embedder = make_rnn(n_layers=3, \n",
        "                    hidden_units=hidden_dim, \n",
        "                    output_units=hidden_dim, \n",
        "                    name='Embedder')\n",
        "recovery = make_rnn(n_layers=3, \n",
        "                    hidden_units=hidden_dim, \n",
        "                    output_units=n_seq, \n",
        "                    name='Recovery')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od4SuDHd7HvL"
      },
      "source": [
        "#### Generator & Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef7Pd44F7HvL"
      },
      "outputs": [],
      "source": [
        "generator = make_rnn(n_layers=3, \n",
        "                     hidden_units=hidden_dim, \n",
        "                     output_units=hidden_dim, \n",
        "                     name='Generator')\n",
        "discriminator = make_rnn(n_layers=3, \n",
        "                         hidden_units=hidden_dim, \n",
        "                         output_units=1, \n",
        "                         name='Discriminator')\n",
        "supervisor = make_rnn(n_layers=2, \n",
        "                      hidden_units=hidden_dim, \n",
        "                      output_units=hidden_dim, \n",
        "                      name='Supervisor')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pb03DEF7HvO"
      },
      "source": [
        "### Training Process\n",
        "\n",
        "Training takes place in three phases:\n",
        "1. Training the autoencoder on real time series to optimize reconstruction\n",
        "2. Optimizing the supervised loss using real time series to capture the temporal\n",
        "dynamics of the historical data\n",
        "3. Jointly training the four components while minimizing all three loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUwB7nrD7HvP"
      },
      "outputs": [],
      "source": [
        "## Losses\n",
        "mse = MeanSquaredError()\n",
        "bce = BinaryCrossentropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzJXfm8s7HvZ"
      },
      "outputs": [],
      "source": [
        "autoencoder_optimizer = Adam()\n",
        "supervisor_optimizer = Adam()\n",
        "generator_optimizer = Adam()\n",
        "discriminator_optimizer = Adam()\n",
        "embedding_optimizer = Adam()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klcHBR-37HvR"
      },
      "source": [
        "#### Phase 1: Autoencoder Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtY_Dm5c7HvT"
      },
      "outputs": [],
      "source": [
        "H = embedder(X)\n",
        "X_tilde = recovery(H)\n",
        "\n",
        "autoencoder = Model(inputs=X,\n",
        "                    outputs=X_tilde,\n",
        "                    name='Autoencoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk3dWMca7HvW",
        "outputId": "d8791ff5-c1c4-49d5-d35c-684243689fe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Autoencoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " RealData (InputLayer)       [(None, 24, 9)]           0         \n",
            "                                                                 \n",
            " Embedder (Sequential)       (None, 24, 24)            10320     \n",
            "                                                                 \n",
            " Recovery (Sequential)       (None, 24, 9)             11025     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,345\n",
            "Trainable params: 21,345\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHtSvz9O7Hvc"
      },
      "outputs": [],
      "source": [
        "@tf.function ##\n",
        "def train_autoencoder_init(x):\n",
        "    with tf.GradientTape() as tape: ##자동미분기능 : 동적으로 gradient값 확인 가능\n",
        "        x_tilde = autoencoder(x)\n",
        "        embedding_loss_t0 = mse(x, x_tilde)\n",
        "        e_loss_0 = 10 * tf.sqrt(embedding_loss_t0)\n",
        "\n",
        "    var_list = embedder.trainable_variables + recovery.trainable_variables\n",
        "    gradients = tape.gradient(e_loss_0, var_list) ##gradient계산\n",
        "    autoencoder_optimizer.apply_gradients(zip(gradients, var_list)) #오차역전파\n",
        "    return tf.sqrt(embedding_loss_t0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfJD_yD17Hu-"
      },
      "outputs": [],
      "source": [
        "writer = tf.summary.create_file_writer(log_dir.as_posix())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMKpLFOR7Hvf",
        "outputId": "5dec2fdc-eed5-4fab-92fd-5b9e323946ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [02:10<00:00, 76.74it/s]\n"
          ]
        }
      ],
      "source": [
        "for step in tqdm(range(train_steps)):\n",
        "    X_ = next(real_series_iter)\n",
        "    step_e_loss_t0 = train_autoencoder_init(X_)\n",
        "    with writer.as_default():\n",
        "        tf.summary.scalar('Loss Autoencoder Init', step_e_loss_t0, step=step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ7K_t647Hvi"
      },
      "source": [
        "### Phase 2: Supervised training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW4OHDHp7Hvk"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_supervisor(x):\n",
        "    with tf.GradientTape() as tape:\n",
        "        h = embedder(x)\n",
        "        h_hat_supervised = supervisor(h)\n",
        "        g_loss_s = mse(h[:, 1:, :], h_hat_supervised[:, :1, :])\n",
        "\n",
        "    var_list = supervisor.trainable_variables\n",
        "    gradients = tape.gradient(g_loss_s, var_list)\n",
        "    supervisor_optimizer.apply_gradients(zip(gradients, var_list))\n",
        "    return g_loss_s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAC0bPno7Hvn",
        "outputId": "6040e6fe-ce55-4fbb-eba4-24fc738be3c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [01:17<00:00, 128.43it/s]\n"
          ]
        }
      ],
      "source": [
        "for step in tqdm(range(train_steps)):\n",
        "    X_ = next(real_series_iter)\n",
        "    step_g_loss_s = train_supervisor(X_)\n",
        "    with writer.as_default():\n",
        "        tf.summary.scalar('Loss Generator Supervised Init', step_g_loss_s, step=step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUAgyoF97Hvr"
      },
      "source": [
        "### Phase 3: Joint Training\n",
        "#### Generator\n",
        "##### Adversarial Architecture - Supervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omhSRq1A7Hvr"
      },
      "outputs": [],
      "source": [
        "E_hat = generator(Z)\n",
        "H_hat = supervisor(E_hat)\n",
        "Y_fake = discriminator(H_hat)\n",
        "\n",
        "adversarial_supervised = Model(inputs=Z,\n",
        "                               outputs=Y_fake,\n",
        "                               name='AdversarialNetSupervised')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsBXZ77A7Hvt",
        "outputId": "f859ed2c-aef8-491c-955e-1a0aa09d35e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"AdversarialNetSupervised\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " RandomData (InputLayer)     [(None, 24, 9)]           0         \n",
            "                                                                 \n",
            " Generator (Sequential)      (None, 24, 24)            10320     \n",
            "                                                                 \n",
            " Supervisor (Sequential)     (None, 24, 24)            7800      \n",
            "                                                                 \n",
            " Discriminator (Sequential)  (None, 24, 1)             10825     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28,945\n",
            "Trainable params: 28,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "adversarial_supervised.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBLM652o7Hvw"
      },
      "source": [
        "##### Adversarial Architecture in Latent Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPBXbWci7Hvw"
      },
      "outputs": [],
      "source": [
        "Y_fake_e = discriminator(E_hat)\n",
        "\n",
        "adversarial_emb = Model(inputs=Z,\n",
        "                    outputs=Y_fake_e,\n",
        "                    name='AdversarialNet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ5iBA6A7Hvy",
        "outputId": "c6900d80-1182-4c9d-bc08-0564cecffc09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"AdversarialNet\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " RandomData (InputLayer)     [(None, 24, 9)]           0         \n",
            "                                                                 \n",
            " Generator (Sequential)      (None, 24, 24)            10320     \n",
            "                                                                 \n",
            " Discriminator (Sequential)  (None, 24, 1)             10825     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,145\n",
            "Trainable params: 21,145\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "adversarial_emb.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avMC_xQ57Hv1"
      },
      "source": [
        "#### Mean & Variance Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-XuPTcq7Hv1"
      },
      "outputs": [],
      "source": [
        "X_hat = recovery(H_hat)\n",
        "synthetic_data = Model(inputs=Z,\n",
        "                       outputs=X_hat,\n",
        "                       name='SyntheticData')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wki450887Hv6",
        "outputId": "811641c7-cb13-4dce-f63f-4af3b48b353b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"SyntheticData\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " RandomData (InputLayer)     [(None, 24, 9)]           0         \n",
            "                                                                 \n",
            " Generator (Sequential)      (None, 24, 24)            10320     \n",
            "                                                                 \n",
            " Supervisor (Sequential)     (None, 24, 24)            7800      \n",
            "                                                                 \n",
            " Recovery (Sequential)       (None, 24, 9)             11025     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29,145\n",
            "Trainable params: 29,145\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "synthetic_data.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HWqLFlp7Hv8"
      },
      "outputs": [],
      "source": [
        "def get_generator_moment_loss(y_true, y_pred):\n",
        "    y_true_mean, y_true_var = tf.nn.moments(x=y_true, axes=[0])\n",
        "    y_pred_mean, y_pred_var = tf.nn.moments(x=y_pred, axes=[0])\n",
        "    g_loss_mean = tf.reduce_mean(tf.abs(y_true_mean - y_pred_mean))\n",
        "    g_loss_var = tf.reduce_mean(tf.abs(tf.sqrt(y_true_var + 1e-6) - tf.sqrt(y_pred_var + 1e-6)))\n",
        "    return g_loss_mean + g_loss_var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs0QSX5B7Hv-"
      },
      "source": [
        "### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkhvkiRN7Hv_"
      },
      "outputs": [],
      "source": [
        "Y_real = discriminator(H)\n",
        "discriminator_model = Model(inputs=X,\n",
        "                            outputs=Y_real,\n",
        "                            name='DiscriminatorReal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u840M9xo7HwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4580b6c-f095-49f3-addc-c4160544d968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"DiscriminatorReal\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " RealData (InputLayer)       [(None, 24, 9)]           0         \n",
            "                                                                 \n",
            " Embedder (Sequential)       (None, 24, 24)            10320     \n",
            "                                                                 \n",
            " Discriminator (Sequential)  (None, 24, 1)             10825     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,145\n",
            "Trainable params: 21,145\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foP4Hbl77HwV"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3I1RNRz7HwI"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_generator(x, z):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_fake = adversarial_supervised(z)\n",
        "        generator_loss_unsupervised = bce(y_true=tf.ones_like(y_fake),\n",
        "                                          y_pred=y_fake)\n",
        "\n",
        "        y_fake_e = adversarial_emb(z)\n",
        "        generator_loss_unsupervised_e = bce(y_true=tf.ones_like(y_fake_e),\n",
        "                                            y_pred=y_fake_e)\n",
        "        h = embedder(x)\n",
        "        h_hat_supervised = supervisor(h)\n",
        "        generator_loss_supervised = mse(h[:, 1:, :], h_hat_supervised[:, :1, :])\n",
        "\n",
        "        x_hat = synthetic_data(z)\n",
        "        generator_moment_loss = get_generator_moment_loss(x, x_hat)\n",
        "\n",
        "        generator_loss = (generator_loss_unsupervised +\n",
        "                          generator_loss_unsupervised_e +\n",
        "                          100 * tf.sqrt(generator_loss_supervised) +\n",
        "                          100 * generator_moment_loss)\n",
        "\n",
        "    var_list = generator.trainable_variables + supervisor.trainable_variables\n",
        "    gradients = tape.gradient(generator_loss, var_list)\n",
        "    generator_optimizer.apply_gradients(zip(gradients, var_list))\n",
        "    return generator_loss_unsupervised, generator_loss_supervised, generator_moment_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9S9vkmC7HwL"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_embedder(x):\n",
        "    with tf.GradientTape() as tape:\n",
        "        h = embedder(x)\n",
        "        h_hat_supervised = supervisor(h)\n",
        "        generator_loss_supervised = mse(h[:, 1:, :], h_hat_supervised[:, :1, :])\n",
        "\n",
        "        x_tilde = autoencoder(x)\n",
        "        embedding_loss_t0 = mse(x, x_tilde)\n",
        "        e_loss = 10 * tf.sqrt(embedding_loss_t0) + 0.1 * generator_loss_supervised\n",
        "\n",
        "    var_list = embedder.trainable_variables + recovery.trainable_variables\n",
        "    gradients = tape.gradient(e_loss, var_list)\n",
        "    embedding_optimizer.apply_gradients(zip(gradients, var_list))\n",
        "    return tf.sqrt(embedding_loss_t0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyKq5OMC7HwO"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def get_discriminator_loss(x, z):\n",
        "    y_real = discriminator_model(x)\n",
        "    discriminator_loss_real = bce(y_true=tf.ones_like(y_real),\n",
        "                                  y_pred=y_real)\n",
        "\n",
        "    y_fake = adversarial_supervised(z)\n",
        "    discriminator_loss_fake = bce(y_true=tf.zeros_like(y_fake),\n",
        "                                  y_pred=y_fake)\n",
        "\n",
        "    y_fake_e = adversarial_emb(z)\n",
        "    discriminator_loss_fake_e = bce(y_true=tf.zeros_like(y_fake_e),\n",
        "                                    y_pred=y_fake_e)\n",
        "    return (discriminator_loss_real +\n",
        "            discriminator_loss_fake +\n",
        "            gamma * discriminator_loss_fake_e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdXEzlqL7HwS"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_discriminator(x, z):\n",
        "    with tf.GradientTape() as tape:\n",
        "        discriminator_loss = get_discriminator_loss(x, z)\n",
        "\n",
        "    var_list = discriminator.trainable_variables\n",
        "    gradients = tape.gradient(discriminator_loss, var_list)\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients, var_list))\n",
        "    return discriminator_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxgwYiuK7HwV",
        "outputId": "0a29fb6f-62c1-439b-9d95-237fe90db30b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     0 | d_loss: 2.2908 | g_loss_u: 0.5509 | g_loss_s: 0.0719 | g_loss_v: 1.5280 | e_loss_t0: 0.8293\n",
            " 1,000 | d_loss: 0.7521 | g_loss_u: 4.5742 | g_loss_s: 0.0200 | g_loss_v: 0.7412 | e_loss_t0: 0.6187\n",
            " 2,000 | d_loss: 0.7129 | g_loss_u: 3.4393 | g_loss_s: 0.0143 | g_loss_v: 0.9259 | e_loss_t0: 0.7450\n",
            " 3,000 | d_loss: 0.4913 | g_loss_u: 3.0039 | g_loss_s: 0.0114 | g_loss_v: 0.9095 | e_loss_t0: 0.7623\n",
            " 4,000 | d_loss: 0.8912 | g_loss_u: 5.3497 | g_loss_s: 0.0093 | g_loss_v: 0.9278 | e_loss_t0: 0.7586\n",
            " 5,000 | d_loss: 0.2500 | g_loss_u: 5.2526 | g_loss_s: 0.0071 | g_loss_v: 0.8662 | e_loss_t0: 0.7485\n",
            " 6,000 | d_loss: 0.6411 | g_loss_u: 4.1594 | g_loss_s: 0.0063 | g_loss_v: 0.9342 | e_loss_t0: 0.7992\n",
            " 7,000 | d_loss: 0.7626 | g_loss_u: 4.0829 | g_loss_s: 0.0061 | g_loss_v: 0.8516 | e_loss_t0: 0.7389\n",
            " 8,000 | d_loss: 0.4697 | g_loss_u: 3.8755 | g_loss_s: 0.0050 | g_loss_v: 0.8386 | e_loss_t0: 0.7155\n",
            " 9,000 | d_loss: 1.2574 | g_loss_u: 6.1405 | g_loss_s: 0.0050 | g_loss_v: 0.8348 | e_loss_t0: 0.7109\n"
          ]
        }
      ],
      "source": [
        "step_g_loss_u = step_g_loss_s = step_g_loss_v = step_e_loss_t0 = step_d_loss = 0\n",
        "for step in range(train_steps):\n",
        "    # Train generator (twice as often as discriminator)\n",
        "    for kk in range(2):\n",
        "        X_ = next(real_series_iter)\n",
        "        Z_ = next(random_series)\n",
        "\n",
        "        # Train generator\n",
        "        step_g_loss_u, step_g_loss_s, step_g_loss_v = train_generator(X_, Z_)\n",
        "        # Train embedder\n",
        "        step_e_loss_t0 = train_embedder(X_)\n",
        "\n",
        "    X_ = next(real_series_iter)\n",
        "    Z_ = next(random_series)\n",
        "    step_d_loss = get_discriminator_loss(X_, Z_)\n",
        "    if step_d_loss > 0.15:\n",
        "        step_d_loss = train_discriminator(X_, Z_)\n",
        "\n",
        "    if step % 1000 == 0:\n",
        "        print(f'{step:6,.0f} | d_loss: {step_d_loss:6.4f} | g_loss_u: {step_g_loss_u:6.4f} | '\n",
        "              f'g_loss_s: {step_g_loss_s:6.4f} | g_loss_v: {step_g_loss_v:6.4f} | e_loss_t0: {step_e_loss_t0:6.4f}')\n",
        "\n",
        "    with writer.as_default():\n",
        "        tf.summary.scalar('G Loss S', step_g_loss_s, step=step)\n",
        "        tf.summary.scalar('G Loss U', step_g_loss_u, step=step)\n",
        "        tf.summary.scalar('G Loss V', step_g_loss_v, step=step)\n",
        "        tf.summary.scalar('E Loss T0', step_e_loss_t0, step=step)\n",
        "        tf.summary.scalar('D Loss', step_d_loss, step=step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwnlUZO37Hwc"
      },
      "source": [
        "#### Generate Synthetic Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPL4KcHf7Hwd"
      },
      "outputs": [],
      "source": [
        "generated_data = []\n",
        "for i in range(int(n_windows / batch_size)):\n",
        "    Z_ = next(random_series)\n",
        "    d = synthetic_data(Z_)\n",
        "    generated_data.append(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e23gvDLX7Hwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1db5cc01-7521-4311-b79a-3a9c511b3bc4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "len(generated_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OppkBPhI7Hwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c37ec91a-0db1-4835-efba-2c60b8dc4c0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(104, 24, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "generated_data = np.array(np.vstack(generated_data))\n",
        "generated_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUnrCaJ27Hwl"
      },
      "outputs": [],
      "source": [
        "np.save(log_dir / 'error_i_portent1.npy', generated_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4eC-s1a7Hwq"
      },
      "source": [
        "### Rescale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG_FCiV77Hwr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72cd3063-b921-4b33-cc92-0841cafda791"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(104, 24, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "generated_data = (scaler.inverse_transform(generated_data.reshape(-1, n_seq)).reshape(-1, seq_len, n_seq))\n",
        "generated_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0ia1IKb7Hwv"
      },
      "outputs": [],
      "source": [
        "with pd.HDFStore(hdf_store) as store:\n",
        "    store.put('data/synthetic', pd.DataFrame(generated_data.reshape(-1, n_seq)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(log_dir / 'portent_p_generated_data_inverse1.npy', generated_data)"
      ],
      "metadata": {
        "id": "0xCK6ccIDoMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/time_gan/experiment_00/eerror_i_portent_generated_data_inverse1.npy', generated_data)"
      ],
      "metadata": {
        "id": "X3Wm5wDWIKlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_data = []\n",
        "for i in range(int(n_windows / batch_size)):\n",
        "    Z_ = next(random_series)\n",
        "    d = synthetic_data(Z_)\n",
        "    generated_data.append(d)\n",
        "\n",
        "generated_data = np.array(np.vstack(generated_data))\n",
        "generated_data = (scaler.inverse_transform(generated_data.reshape(-1, n_seq)).reshape(-1, seq_len, n_seq))\n",
        "\n",
        "np.save('/content/drive/MyDrive/time_gan/experiment_00/eerror_i_portent_generated_data_inverse2.npy', generated_data)\n",
        "\n",
        "generated_data = []\n",
        "for i in range(int(n_windows / batch_size)):\n",
        "    Z_ = next(random_series)\n",
        "    d = synthetic_data(Z_)\n",
        "    generated_data.append(d)\n",
        "\n",
        "generated_data = np.array(np.vstack(generated_data))\n",
        "generated_data = (scaler.inverse_transform(generated_data.reshape(-1, n_seq)).reshape(-1, seq_len, n_seq))\n",
        "\n",
        "np.save('/content/drive/MyDrive/time_gan/experiment_00/eerror_i_portent_generated_data_inverse3.npy', generated_data)\n",
        "\n",
        "generated_data = []\n",
        "for i in range(int(n_windows / batch_size)):\n",
        "    Z_ = next(random_series)\n",
        "    d = synthetic_data(Z_)\n",
        "    generated_data.append(d)\n",
        "\n",
        "generated_data = np.array(np.vstack(generated_data))\n",
        "generated_data = (scaler.inverse_transform(generated_data.reshape(-1, n_seq)).reshape(-1, seq_len, n_seq))\n",
        "\n",
        "np.save('/content/drive/MyDrive/time_gan/experiment_00/eerror_i_portent_generated_data_inverse4.npy', generated_data)"
      ],
      "metadata": {
        "id": "3zgfoZIN3Uao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UAIiznsNd1G",
        "outputId": "d067fa49-233b-4aa5-a64d-8ab8e869e6a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(104, 24, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "COjHc9OXNs09",
        "outputId": "b5802317-c0b3-47c4-e356-4e613d91a851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-9458282bfa6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msynthetic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'synthetic' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKsO3hL47Hww"
      },
      "source": [
        "#### Plot sample Series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkBfOQ5o7Hwy",
        "outputId": "921d34d4-0e86-4470-a4db-1e8482013fb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-4423d53516d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     (pd.DataFrame({'Real': real[:, j],\n\u001b[0;32m---> 14\u001b[0;31m                    'Synthetic': synthetic[:, j]})\n\u001b[0m\u001b[1;32m     15\u001b[0m      .plot(ax=axes[j],\n\u001b[1;32m     16\u001b[0m            \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     return arrays_to_mgr(\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m             val = sanitize_array(\n\u001b[0;32m--> 590\u001b[0;31m                 \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m             )\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure, allow_2d)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_infer_to_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m     \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_ndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36m_sanitize_ndim\u001b[0;34m(result, data, dtype, index, allow_2d)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data must be 1-dimensional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtensionDtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;31m# i.e. PandasDtype(\"O\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data must be 1-dimensional"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x504 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#tickers=['Ia','Ib','Ic','Idc','RPM','Torque','Ia_RMS','Ib_RMS','IC_RMS']\n",
        "tickers=cols\n",
        "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(14, 7))\n",
        "axes = axes.flatten()\n",
        "\n",
        "index = list(range(1, 10))\n",
        "synthetic = generated_data[np.random.randint(n_windows)]\n",
        "\n",
        "idx = np.random.randint(len(Xp) - seq_len)\n",
        "real = Xp[idx: idx + seq_len]\n",
        "\n",
        "for j, ticker in enumerate(tickers):\n",
        "    (pd.DataFrame({'Real': real[:, j],\n",
        "                   'Synthetic': synthetic[:, j]})\n",
        "     .plot(ax=axes[j],\n",
        "           title=ticker,\n",
        "           secondary_y='Synthetic', style=['-', '--'],\n",
        "           lw=1))\n",
        "sns.despine()\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX3nyKRqnIvS"
      },
      "source": [
        "### Evaluation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVF-p8R67Hw0"
      },
      "outputs": [],
      "source": [
        "def get_real_data(dataset):\n",
        "\n",
        "    # Preprocess the dataset:\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(dataset)\n",
        "\n",
        "    data = []\n",
        "    for i in range(len(dataset) - seq_len):\n",
        "        data.append(scaled_data[i:i + seq_len])\n",
        "    return data\n",
        "\n",
        "\n",
        "real_data = get_real_data(Xp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc-Ue9w97Hw4"
      },
      "outputs": [],
      "source": [
        "real_data = real_data[:generated_data.shape[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBub1F8c7Hw8"
      },
      "outputs": [],
      "source": [
        "sample_size = 50\n",
        "idx = np.random.permutation(len(real_data))[:sample_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdcORJzJ7Hw-"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "real_sample = np.asarray(real_data)[idx]\n",
        "synthetic_sample = np.asarray(generated_data)[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ua1qNYAC7HxA"
      },
      "outputs": [],
      "source": [
        "real_sample_2d = real_sample.reshape(-1, seq_len)\n",
        "synthetic_sample_2d = synthetic_sample.reshape(-1, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YEkw8zR7HxD"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d14_kkFY7HxF"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca.fit(real_sample_2d)\n",
        "pca_real = (pd.DataFrame(pca.transform(real_sample_2d))\n",
        "            .assign(Data='Real'))\n",
        "pca_synthetic = (pd.DataFrame(pca.transform(synthetic_sample_2d))\n",
        "                 .assign(Data='Synthetic'))\n",
        "pca_result = pca_real.append(pca_synthetic).rename(\n",
        "    columns={0: '1st Component', 1: '2nd Component'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOLjMsou7HxG"
      },
      "outputs": [],
      "source": [
        "tsne_data = np.concatenate((real_sample_2d,\n",
        "                            synthetic_sample_2d), axis=0)\n",
        "\n",
        "tsne = TSNE(n_components=2,\n",
        "            verbose=1,\n",
        "            perplexity=40)\n",
        "tsne_result = tsne.fit_transform(tsne_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Yv2PS_e7HxI"
      },
      "outputs": [],
      "source": [
        "tsne_result = pd.DataFrame(tsne_result, columns=['X', 'Y']).assign(Data='Real')\n",
        "tsne_result.loc[sample_size*6:, 'Data'] = 'Synthetic'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta1e7ZIo7HxK"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(ncols=2, figsize=(14, 5))\n",
        "\n",
        "sns.scatterplot(x='1st Component', y='2nd Component', data=pca_result,\n",
        "                hue='Data', style='Data', ax=axes[0])\n",
        "sns.despine()\n",
        "axes[0].set_title('PCA Result')\n",
        "\n",
        "\n",
        "sns.scatterplot(x='X', y='Y',\n",
        "                data=tsne_result,\n",
        "                hue='Data', \n",
        "                style='Data', \n",
        "                ax=axes[1])\n",
        "sns.despine()\n",
        "for i in [0, 1]:\n",
        "    axes[i].set_xticks([])\n",
        "    axes[i].set_yticks([])\n",
        "\n",
        "axes[1].set_title('t-SNE Result')\n",
        "fig.suptitle('Assessing Diversity: Qualitative Comparison of Real and Synthetic Data Distributions', \n",
        "             fontsize=14)\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(top=.88);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCXFYq56nM3s"
      },
      "source": [
        "#### Refrences:\n",
        "\n",
        "Book: Machine Learning for algorithmic trading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#%% Post-hoc RNN Classifier \n",
        "\n",
        "def discriminative_score_metrics (dataX, dataX_hat):\n",
        "  \n",
        "    # Initialization on the Graph\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # Basic Parameters\n",
        "    No = len(dataX)\n",
        "    data_dim = len(dataX[0][0,:])\n",
        "    \n",
        "    # Compute Maximum seq length and each seq length\n",
        "    dataT = list()\n",
        "    Max_Seq_Len = 0\n",
        "    for i in range(No):\n",
        "        Max_Seq_Len = max(Max_Seq_Len, len(dataX[i][:,0]))\n",
        "        dataT.append(len(dataX[i][:,0]))\n",
        "     \n",
        "    # Network Parameters\n",
        "    hidden_dim = max(int(data_dim/2),1)\n",
        "    iterations = 2000\n",
        "    batch_size = 128\n",
        "    \n",
        "    #%% input place holders\n",
        "    # Features\n",
        "    X = tf.placeholder(tf.float32, [None, Max_Seq_Len, data_dim], name = \"myinput_x\")\n",
        "    X_hat = tf.placeholder(tf.float32, [None, Max_Seq_Len, data_dim], name = \"myinput_x_hat\")\n",
        "    \n",
        "    # Times\n",
        "    T = tf.placeholder(tf.int32, [None], name = \"myinput_t\")\n",
        "    T_hat = tf.placeholder(tf.int32, [None], name = \"myinput_t_hat\")\n",
        "    \n",
        "    #%% builde a RNN classification network \n",
        "    \n",
        "    def discriminator (X, T):\n",
        "      \n",
        "        with tf.variable_scope(\"discriminator\", reuse = tf.AUTO_REUSE) as vs:\n",
        "            \n",
        "            d_cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim, activation=tf.nn.tanh, name = 'cd_cell')\n",
        "                    \n",
        "            d_outputs, d_last_states = tf.nn.dynamic_rnn(d_cell, X, dtype=tf.float32, sequence_length = T)\n",
        "                \n",
        "            # Logits\n",
        "            Y_hat = tf.contrib.layers.fully_connected(d_last_states, 1, activation_fn=None) \n",
        "            \n",
        "            # Sigmoid output\n",
        "            Y_hat_Final = tf.nn.sigmoid(Y_hat)\n",
        "            \n",
        "            # Variables\n",
        "            d_vars = [v for v in tf.all_variables() if v.name.startswith(vs.name)]\n",
        "    \n",
        "        return Y_hat, Y_hat_Final, d_vars\n",
        "    \n",
        "    #%% Train / Test Division\n",
        "    def train_test_divide (dataX, dataX_hat, dataT):\n",
        "      \n",
        "        # Divide train/test index\n",
        "        No = len(dataX)\n",
        "        idx = np.random.permutation(No)\n",
        "        train_idx = idx[:int(No*0.8)]\n",
        "        test_idx = idx[int(No*0.8):]\n",
        "        \n",
        "        # Train and Test X\n",
        "        trainX = [dataX[i] for i in train_idx]\n",
        "        trainX_hat = [dataX_hat[i] for i in train_idx]\n",
        "        \n",
        "        testX = [dataX[i] for i in test_idx]\n",
        "        testX_hat = [dataX_hat[i] for i in test_idx]\n",
        "        \n",
        "        # Train and Test T\n",
        "        trainT = [dataT[i] for i in train_idx]\n",
        "        testT = [dataT[i] for i in test_idx]\n",
        "      \n",
        "        return trainX, trainX_hat, testX, testX_hat, trainT, testT\n",
        "    \n",
        "    #%% Functions\n",
        "    # Variables\n",
        "    Y_real, Y_pred_real, d_vars = discriminator(X, T)\n",
        "    Y_fake, Y_pred_fake, _ = discriminator(X_hat, T_hat)\n",
        "        \n",
        "    # Loss for the discriminator\n",
        "    D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Y_real, labels = tf.ones_like(Y_real)))\n",
        "    D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Y_fake, labels = tf.zeros_like(Y_fake)))\n",
        "    D_loss = D_loss_real + D_loss_fake\n",
        "    \n",
        "    # optimizer\n",
        "    D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list = d_vars)\n",
        "        \n",
        "    #%% Sessions    \n",
        "\n",
        "    # Start session and initialize\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Train / Test Division\n",
        "    trainX, trainX_hat, testX, testX_hat, trainT, testT = train_test_divide (dataX, dataX_hat, dataT)\n",
        "    \n",
        "    # Training step\n",
        "    for itt in range(iterations):\n",
        "          \n",
        "        # Batch setting\n",
        "        idx = np.random.permutation(len(trainX))\n",
        "        train_idx = idx[:batch_size]     \n",
        "            \n",
        "        X_mb = list(trainX[i] for i in train_idx)\n",
        "        T_mb = list(trainT[i] for i in train_idx)\n",
        "        \n",
        "        # Batch setting\n",
        "        idx = np.random.permutation(len(trainX_hat))\n",
        "        train_idx = idx[:batch_size]     \n",
        "            \n",
        "        X_hat_mb = list(trainX_hat[i] for i in train_idx)\n",
        "        T_hat_mb = list(trainT[i] for i in train_idx)\n",
        "          \n",
        "        # Train discriminator\n",
        "        _, step_d_loss = sess.run([D_solver, D_loss], feed_dict={X: X_mb, T: T_mb, X_hat: X_hat_mb, T_hat: T_hat_mb})            \n",
        "        \n",
        "        #%% Checkpoints\n",
        "#        if itt % 500 == 0:\n",
        "#            print(\"[step: {}] loss - d loss: {}\".format(itt, np.round(step_d_loss,4)))\n",
        "    \n",
        "    #%% Final Outputs (ontTesting set)\n",
        "    \n",
        "    Y_pred_real_curr, Y_pred_fake_curr = sess.run([Y_pred_real, Y_pred_fake], feed_dict={X: testX, T: testT, X_hat: testX_hat, T_hat: testT})\n",
        "    \n",
        "    Y_pred_final = np.squeeze(np.concatenate((Y_pred_real_curr, Y_pred_fake_curr), axis = 0))\n",
        "    Y_label_final = np.concatenate((np.ones([len(Y_pred_real_curr),]), np.zeros([len(Y_pred_real_curr),])), axis = 0)\n",
        "    \n",
        "    #%% Accuracy\n",
        "    Acc = accuracy_score(Y_label_final, Y_pred_final>0.5)\n",
        "    \n",
        "    Disc_Score = np.abs(0.5-Acc)\n",
        "    \n",
        "    return Disc_Score"
      ],
      "metadata": {
        "id": "t0s8GnVhxpWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "#%% Post-hoc RNN one-step ahead predictor\n",
        "\n",
        "def predictive_score_metrics (dataX, dataX_hat):\n",
        "  \n",
        "    # Initialization on the Graph\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # Basic Parameters\n",
        "    No = len(dataX)\n",
        "    data_dim = len(dataX[0][0,:])\n",
        "    \n",
        "    # Maximum seq length and each seq length\n",
        "    dataT = list()\n",
        "    Max_Seq_Len = 0\n",
        "    for i in range(No):\n",
        "        Max_Seq_Len = max(Max_Seq_Len, len(dataX[i][:,0]))\n",
        "        dataT.append(len(dataX[i][:,0]))\n",
        "     \n",
        "    # Network Parameters\n",
        "    hidden_dim = max(int(data_dim/2),1)\n",
        "    iterations = 5000\n",
        "    batch_size = 128\n",
        "    \n",
        "    #%% input place holders\n",
        "    \n",
        "    X = tf.placeholder(tf.float32, [None, Max_Seq_Len-1, data_dim-1], name = \"myinput_x\")\n",
        "    T = tf.placeholder(tf.int32, [None], name = \"myinput_t\")    \n",
        "    Y = tf.placeholder(tf.float32, [None, Max_Seq_Len-1, 1], name = \"myinput_y\")\n",
        "    \n",
        "    #%% builde a RNN discriminator network \n",
        "    \n",
        "    def predictor (X, T):\n",
        "      \n",
        "        with tf.variable_scope(\"predictor\", reuse = tf.AUTO_REUSE) as vs:\n",
        "            \n",
        "            d_cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim, activation=tf.nn.tanh, name = 'd_cell')\n",
        "                    \n",
        "            d_outputs, d_last_states = tf.nn.dynamic_rnn(d_cell, X, dtype=tf.float32, sequence_length = T)\n",
        "                \n",
        "            Y_hat = tf.contrib.layers.fully_connected(d_outputs, 1, activation_fn=None) \n",
        "            \n",
        "            Y_hat_Final = tf.nn.sigmoid(Y_hat)\n",
        "            \n",
        "            d_vars = [v for v in tf.all_variables() if v.name.startswith(vs.name)]\n",
        "    \n",
        "        return Y_hat_Final, d_vars\n",
        "    \n",
        "    #%% Functions\n",
        "    # Variables\n",
        "    Y_pred, d_vars = predictor(X, T)\n",
        "        \n",
        "    # Loss for the predictor\n",
        "    D_loss = tf.losses.absolute_difference(Y, Y_pred)\n",
        "    \n",
        "    # optimizer\n",
        "    D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list = d_vars)\n",
        "        \n",
        "    #%% Sessions    \n",
        "\n",
        "    # Session start\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Training using Synthetic dataset\n",
        "    for itt in range(iterations):\n",
        "          \n",
        "        # Batch setting\n",
        "        idx = np.random.permutation(len(dataX_hat))\n",
        "        train_idx = idx[:batch_size]     \n",
        "            \n",
        "        X_mb = list(dataX_hat[i][:-1,:(data_dim-1)] for i in train_idx)\n",
        "        T_mb = list(dataT[i]-1 for i in train_idx)\n",
        "        Y_mb = list(np.reshape(dataX_hat[i][1:,(data_dim-1)],[len(dataX_hat[i][1:,(data_dim-1)]),1]) for i in train_idx)        \n",
        "          \n",
        "        # Train discriminator\n",
        "        _, step_d_loss = sess.run([D_solver, D_loss], feed_dict={X: X_mb, T: T_mb, Y: Y_mb})            \n",
        "        \n",
        "        #%% Checkpoints\n",
        "#        if itt % 500 == 0:\n",
        "#            print(\"[step: {}] loss - d loss: {}\".format(itt, np.sqrt(np.round(step_d_loss,4))))\n",
        "    \n",
        "    #%% Use Original Dataset to test\n",
        "    \n",
        "    # Make Batch with Original Data\n",
        "    idx = np.random.permutation(len(dataX_hat))\n",
        "    train_idx = idx[:No]     \n",
        "    \n",
        "    X_mb = list(dataX[i][:-1,:(data_dim-1)] for i in train_idx)\n",
        "    T_mb = list(dataT[i]-1 for i in train_idx)\n",
        "    Y_mb = list(np.reshape(dataX[i][1:,(data_dim-1)], [len(dataX[i][1:,(data_dim-1)]),1]) for i in train_idx)\n",
        "    \n",
        "    # Predict Fugure\n",
        "    pred_Y_curr = sess.run(Y_pred, feed_dict={X: X_mb, T: T_mb})\n",
        "    \n",
        "    # Compute MAE\n",
        "    MAE_Temp = 0\n",
        "    for i in range(No):\n",
        "        MAE_Temp = MAE_Temp + mean_absolute_error(Y_mb[i], pred_Y_curr[i,:,:])\n",
        "    \n",
        "    MAE = MAE_Temp / No\n",
        "    \n",
        "    return MAE"
      ],
      "metadata": {
        "id": "WxWxdzPtxwHD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}